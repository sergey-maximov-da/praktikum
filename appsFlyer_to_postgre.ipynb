{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доспуп к Эластику \t True\n",
      "Доспуп к Монге \t\t True\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.parse import unquote\n",
    "import json\n",
    "from datetime import date, datetime, timedelta\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "from pymongo import MongoClient\n",
    "from elasticsearch import Elasticsearch\n",
    "from basic.claims import connect #, get_claims, get_users, get_kibana_gate, products_bankiros, nulls, get_medium, get_claims_raw\n",
    "# from basic.external import cpa_parse, get_yandex_data\n",
    "# from basic.dscore import cre_get, mfo_score\n",
    "from basic.reports import cre_keyword_report, master_report, registrations_medium\n",
    "from basic.vars import regions_mapper, claim_offers\n",
    "from bson.objectid import ObjectId\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, date, timedelta\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from base64 import b64encode\n",
    "import numpy as np\n",
    "# from robobrowser import RoboBrowser\n",
    "import re\n",
    "# import csv, sqlite3\n",
    "# import sqlalchemy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from calendar import monthrange\n",
    "import pickle\n",
    "import os.path\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from googleapiclient.discovery import build\n",
    "from dateutil.relativedelta import relativedelta\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "# from IPython.display import Filelink, FileLinks\n",
    "\n",
    "\n",
    "client, es = connect()\n",
    "\n",
    "\n",
    "def get_appsf_data(app_id, report_type, since, till):\n",
    "    '''\n",
    "    since and till should be in datetime.date format\n",
    "    Minimum date is 2021-07-07\n",
    "    app_id: 'id1540063555' 'ru.odobrim.client'\n",
    "    report types : 'in-app-events', 'installs_report', 'organic_in_app_events_report', 'organic_installs_report'\n",
    "    '''\n",
    "    api_token = '3ad172db-cc3c-4c02-931d-d653aa02d175'\n",
    "    df = pd.DataFrame()\n",
    "    days_limit = 30\n",
    "    if (date.today() - since).days >90:\n",
    "        since2 = date.today() - timedelta(days=90)\n",
    "    else:\n",
    "        since2 = since\n",
    "    while since2 < till:\n",
    "        if (till - since2).days > days_limit:\n",
    "            till2 = since2+timedelta(days=days_limit)\n",
    "        else:\n",
    "            till2 = till\n",
    "        print(since2, till2, report_type)\n",
    "        params = {\n",
    "          'api_token': api_token,\n",
    "          'from': f'{since2}',\n",
    "          'to': f'{till2}'\n",
    "        }\n",
    "#         print(params)\n",
    "        request_url = f'http://hq.appsflyer.com/export/{app_id}/{report_type}/v5'\n",
    "        while True:\n",
    "            try:\n",
    "                res = requests.get(request_url, params=params)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e, e.args)\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            if res.status_code == 404:\n",
    "                print('There is a problem with the request URL. Make sure that it is correct')\n",
    "            else:\n",
    "                print('There was a problem retrieving data: ', res.text)\n",
    "        else:\n",
    "            df1 = pd.read_csv(StringIO(res.text))\n",
    "            df = pd.concat([df, df1])\n",
    "        since2 = since2+timedelta(days=days_limit)\n",
    "\n",
    "    if report_type in ['in-app-events', 'organic_in_app_events_report', 'in_app_events_report']:\n",
    "        df['Event Value'] = df['Event Value'].apply(lambda x: json.loads(x.replace(':userId', '\"userId\":').replace(':claimId', '\"claimId\":')))\n",
    "        event_params = ([list(a.keys()) for a in df['Event Value'].tolist()])\n",
    "        event_params = list(set(item for sublist in event_params for item in sublist))\n",
    "        for param in event_params:\n",
    "            df[param] = df['Event Value'].apply(lambda x: x[param] if param in x.keys() else np.nan)\n",
    "    return df.dropna(axis=1, how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "since, till = date(2021, 1, 1), date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "# try:\n",
    "#     # Подключение к существующей базе данных\n",
    "#     connection = psycopg2.connect(user=\"postgres\",\n",
    "#                                   # пароль, который указали при установке PostgreSQL\n",
    "#                                   password=\"postgres\",\n",
    "#                                   host=\"postgresql-data-superset\",\n",
    "#                                   port=\"5432\")\n",
    "#     connection.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n",
    "#     # Курсор для выполнения операций с базой данных\n",
    "#     cursor = connection.cursor()\n",
    "#     sql_create_database = 'create database appsflyer'\n",
    "#     cursor.execute(sql_create_database)\n",
    "# except (Exception, Error) as error:\n",
    "#     print(\"Ошибка при работе с PostgreSQL\", error)\n",
    "# finally:\n",
    "#     if connection:\n",
    "#         cursor.close()\n",
    "#         connection.close()\n",
    "#         print(\"Соединение с PostgreSQL закрыто\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql+psycopg2://postgres:postgres@postgresql-superset-headless.creditmarkt.svc:5432/postgres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2.extras import Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-21 2021-10-21 installs_report\n",
      "2021-10-21 2021-11-20 installs_report\n",
      "2021-11-20 2021-12-20 installs_report\n",
      "2021-09-21 2021-10-21 in-app-events\n",
      "2021-10-21 2021-11-20 in-app-events\n",
      "2021-11-20 2021-12-20 in-app-events\n",
      "2021-09-21 2021-10-21 organic_installs_report\n",
      "2021-10-21 2021-11-20 organic_installs_report\n",
      "2021-11-20 2021-12-20 organic_installs_report\n",
      "2021-09-21 2021-10-21 organic_in_app_events_report\n",
      "2021-10-21 2021-11-20 organic_in_app_events_report\n",
      "2021-11-20 2021-12-20 organic_in_app_events_report\n"
     ]
    }
   ],
   "source": [
    "client = 'id1540063555'\n",
    "#client = 'ru.odobrim.client'\n",
    "\n",
    "di = get_appsf_data(client, 'installs_report', since, till)\n",
    "df = get_appsf_data(client, 'in-app-events', since, till)\n",
    "del(df['Event Value'])\n",
    "dio = get_appsf_data(client, 'organic_installs_report', since, till)\n",
    "dfo = get_appsf_data(client, 'organic_in_app_events_report', since, till)\n",
    "del(dfo['Event Value'])\n",
    "\n",
    "di = pd.concat([di, dio])\n",
    "df = pd.concat([df, dfo])\n",
    "\n",
    "du = df[df['userId'].notnull()][['AppsFlyer ID', 'userId']].copy()\n",
    "du.index = du['AppsFlyer ID']\n",
    "users = du['userId'].to_dict()\n",
    "di['userId'] = di['AppsFlyer ID'].map(users).fillna('Not assigned')\n",
    "df['userId'] = df['AppsFlyer ID'].map(users).fillna('Not assigned')\n",
    "\n",
    "df2 = pd.concat([di, df])\n",
    "df2['Media Source'] = df2['Media Source'].fillna('Organic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "#engine = create_engine('postgresql+psycopg2://postgres:postgres@postgresql-data-superset:5432/postgres')\n",
    "#df2.columns=df2.columns.apply(lambda x: x.lower())\n",
    "df2.columns = [x.lower() for x in df2.columns]\n",
    "df2.columns = df2.columns.str.replace(' ', '_')\n",
    "df2\n",
    "df2.to_sql('appsflyer', engine, if_exists='append')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-09-21 2021-10-21 installs_report\n",
      "2021-10-21 2021-11-20 installs_report\n",
      "2021-11-20 2021-12-20 installs_report\n",
      "2021-09-21 2021-10-21 in-app-events\n",
      "HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_OKDRAGEGTFLEMLH (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response'))) (MaxRetryError(\"HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_OKDRAGEGTFLEMLH (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))\"),)\n",
      "HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_AZICTBTDATDWAZF (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response'))) (MaxRetryError(\"HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_AZICTBTDATDWAZF (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))\"),)\n",
      "HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_GWJMSHZBPCKXDZZ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response'))) (MaxRetryError(\"HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_GWJMSHZBPCKXDZZ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))\"),)\n",
      "HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_YMNXLASQZNPXVYH (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response'))) (MaxRetryError(\"HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_YMNXLASQZNPXVYH (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))\"),)\n",
      "HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_IMSDDROQKOZYUUZ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response'))) (MaxRetryError(\"HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_IMSDDROQKOZYUUZ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))\"),)\n",
      "HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_PNKIHPFZAGGJKQI (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response'))) (MaxRetryError(\"HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_PNKIHPFZAGGJKQI (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))\"),)\n",
      "HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_WEHQVWDVCRCMJEX (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response'))) (MaxRetryError(\"HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_WEHQVWDVCRCMJEX (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))\"),)\n",
      "HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_FGTMPLQQRWFIKKN (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response'))) (MaxRetryError(\"HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_FGTMPLQQRWFIKKN (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))\"),)\n",
      "HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_GMZKQARTQCPGEYH (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response'))) (MaxRetryError(\"HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_GMZKQARTQCPGEYH (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))\"),)\n",
      "HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_FDDNUAAEIJODODB (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response'))) (MaxRetryError(\"HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_FDDNUAAEIJODODB (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))\"),)\n",
      "HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_AHYJJFKEOVEICLW (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response'))) (MaxRetryError(\"HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_AHYJJFKEOVEICLW (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))\"),)\n",
      "HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_DWBCIJYZOZPHYDJ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response'))) (MaxRetryError(\"HTTPSConnectionPool(host='rawdata.appsflyer.com', port=443): Max retries exceeded with url: /export/token/lego_test__non_organic_in_app_events_2021-09-21_2021-10-21_DWBCIJYZOZPHYDJ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))\"),)\n",
      "There was a problem retrieving data:  Your API calls limit has been reached for report type - In App Events. https://support.appsflyer.com/hc/en-us/articles/207034366-API-Policy\n",
      "2021-10-21 2021-11-20 in-app-events\n",
      "There was a problem retrieving data:  Your API calls limit has been reached for report type - In App Events. https://support.appsflyer.com/hc/en-us/articles/207034366-API-Policy\n",
      "2021-11-20 2021-12-20 in-app-events\n",
      "There was a problem retrieving data:  Your API calls limit has been reached for report type - In App Events. https://support.appsflyer.com/hc/en-us/articles/207034366-API-Policy\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Event Value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Event Value'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-aecf36fea349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_appsf_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'installs_report'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msince\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_appsf_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in-app-events'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msince\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Event Value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_appsf_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'organic_installs_report'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msince\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-05fd32492314>\u001b[0m in \u001b[0;36mget_appsf_data\u001b[0;34m(app_id, report_type, since, till)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreport_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'in-app-events'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'organic_in_app_events_report'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in_app_events_report'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Event Value'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Event Value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':userId'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\"userId\":'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':claimId'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\"claimId\":'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mevent_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Event Value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mevent_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevent_params\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Event Value'"
     ]
    }
   ],
   "source": [
    "#client = 'id1540063555'\n",
    "client = 'ru.odobrim.client'\n",
    "\n",
    "di = get_appsf_data(client, 'installs_report', since, till)\n",
    "df = get_appsf_data(client, 'in-app-events', since, till)\n",
    "del(df['Event Value'])\n",
    "dio = get_appsf_data(client, 'organic_installs_report', since, till)\n",
    "dfo = get_appsf_data(client, 'organic_in_app_events_report', since, till)\n",
    "del(dfo['Event Value'])\n",
    "\n",
    "di = pd.concat([di, dio])\n",
    "df = pd.concat([df, dfo])\n",
    "\n",
    "du = df[df['userId'].notnull()][['AppsFlyer ID', 'userId']].copy()\n",
    "du.index = du['AppsFlyer ID']\n",
    "users = du['userId'].to_dict()\n",
    "di['userId'] = di['AppsFlyer ID'].map(users).fillna('Not assigned')\n",
    "df['userId'] = df['AppsFlyer ID'].map(users).fillna('Not assigned')\n",
    "\n",
    "df2 = pd.concat([di, df])\n",
    "df2['Media Source'] = df2['Media Source'].fillna('Organic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "#engine = create_engine('postgresql+psycopg2://postgres:postgres@postgresql-data-superset:5432/postgres')\n",
    "#df2.columns=df2.columns.apply(lambda x: x.lower())\n",
    "df2.columns = [x.lower() for x in df2.columns]\n",
    "df2.columns = df2.columns.str.replace(' ', '_')\n",
    "df2\n",
    "df2.to_sql('appsflyer_android', engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_postgres(query):\n",
    "    try:\n",
    "        # Подключиться к существующей базе данных\n",
    "        connection = psycopg2.connect(user=\"postgres\",\n",
    "                                  # пароль, который указали при установке PostgreSQL\n",
    "                                  password=\"postgres\",\n",
    "                                  host=\"postgresql-data-superset/postgres\",\n",
    "                                  port=\"5432\")\n",
    "\n",
    "        cursor = connection.cursor()\n",
    "        postgresql_select_query = query\n",
    "\n",
    "        cursor.execute(postgresql_select_query)\n",
    "        mobile_records = cursor.fetchall()\n",
    "        print(pd.DataFrame(mobile_records))\n",
    "\n",
    "    except (Exception, Error) as error:\n",
    "        print(\"Ошибка при работе с PostgreSQL\", error)\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"Соединение с PostgreSQL закрыто\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = '''\n",
    "# select \n",
    "#     platform,campaign,media_source,event_name,sum(table_name.claim_count)\n",
    "# from \n",
    "#    (SELECT  appsflyer.platform,\n",
    "#             appsflyer.campaign,\n",
    "#             appsflyer.event_name,\n",
    "#             appsflyer.media_source,\n",
    "#             appsflyer.event_time, \n",
    "#             count(userid) as claim_count\n",
    "#     FROM appsflyer\n",
    "#     group by platform,campaign,event_name,media_source,event_time) as table_name\n",
    "    \n",
    "# Where event_time > '2021-09-01'\n",
    "# group by platform,campaign,media_source,event_name\n",
    "\n",
    "# UNION all\n",
    "\n",
    "# select platform,campaign,media_source,event_name,sum(table_name1.claim_count)\n",
    "# from \n",
    "#    (SELECT  platform,campaign,media_source,event_name,event_time, count(userid) as claim_count\n",
    "#     FROM appsflyer_android\n",
    "#     group by platform,campaign,media_source,event_name,event_time) as table_name1\n",
    "# Where event_time > '2021-09-01'\n",
    "# group by platform,campaign,event_name,media_source\n",
    "\n",
    "# '''\n",
    "# get_postgres(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gspread_dataframe\n",
    "\n",
    "def gsheet_api_check(SCOPES):\n",
    "    creds = None\n",
    "    if os.path.exists('token.pickle'):\n",
    "        with open('token.pickle', 'rb') as token:\n",
    "            creds = pickle.load(token)    \n",
    "            \n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)  # сюда json файл если он не в той же папке\n",
    "            creds = flow.run_local_server(port=0)        \n",
    "            \n",
    "        with open('token.pickle', 'wb') as token:\n",
    "            pickle.dump(creds, token)    \n",
    "    return creds\n",
    "\n",
    "def pull_sheet_data(SCOPES,SPREADSHEET_ID,RANGE_NAME):\n",
    "    creds = gsheet_api_check(SCOPES)\n",
    "    service = build('sheets', 'v4', credentials=creds)\n",
    "    sheet = service.spreadsheets()\n",
    "    result = sheet.values().get(\n",
    "        spreadsheetId=SPREADSHEET_ID,\n",
    "        range=RANGE_NAME).execute()\n",
    "    values = result.get('values', [])\n",
    "    \n",
    "    if not values:\n",
    "        print('No data found.')\n",
    "    else:\n",
    "        rows = sheet.values().get(spreadsheetId=SPREADSHEET_ID,\n",
    "                                  range=RANGE_NAME).execute()\n",
    "        data = rows.get('values')\n",
    "        print(\"COMPLETE: Data copied\")\n",
    "        return data\n",
    "    \n",
    "def to_float(x):\n",
    "    try:\n",
    "        return(float(x.replace(\"\\xa0\", '').replace(\" \", '').strip()))\n",
    "    except Exception as e:\n",
    "        return np.nan\n",
    "\n",
    "def cre_get(es, since):\n",
    "    \n",
    "    chunk = 10000\n",
    "    gte = (str(datetime.combine(since, datetime.min.time()).timestamp()).replace(\".\", \"\")+\"0000000\")[:13]\n",
    "    payload = {\"size\": chunk,\n",
    "               \"_source\": [\"params.parsed\"],\n",
    "\"query\":{\"bool\":{\"must\":[{\"match_all\":{}},\n",
    "                         {\"match_phrase\":{\"eventType.keyword\": \"CRE\"}},\n",
    "                         {\"range\":{\"timestamp\":{\"gte\":gte,\"format\":\"epoch_millis\"}}}\n",
    "                        ]}},\n",
    "\"_source\": [\"params.parsed\", \"timestamp\"]\n",
    "              }\n",
    "    \n",
    "    d = es.search(index='events', doc_type='event', body=payload, scroll='120m', request_timeout=3000)\n",
    "    sleep(0.5)\n",
    "    doc_count = d['hits']['total']\n",
    "    hits = d['hits']['hits']\n",
    "    print(doc_count, end =\"; \", flush=True)\n",
    "    res = []\n",
    "    for i in range(1, int(doc_count/chunk)+1):\n",
    "        scroll = d['_scroll_id']\n",
    "        d = es.scroll(scroll_id = scroll, scroll = '120m', request_timeout=3000)\n",
    "        hits = hits + d['hits']['hits']\n",
    "        print(doc_count-chunk*i, end =\"; \", flush=True)\n",
    "        sleep(0.5)\n",
    "#         break\n",
    "    print(len(hits), 'raz')\n",
    "    cre = []\n",
    "    for h in hits:\n",
    "        if len(h['_source']['params']['parsed'])>10:\n",
    "            h2 = json.loads(h['_source']['params']['parsed'].replace(\"'\", '\"'))\n",
    "            h2.update({\"timestamp\": pd.to_datetime(h['_source']['timestamp'])})\n",
    "            cre.append(h2)\n",
    "    df = pd.DataFrame(cre).sort_values(\"timestamp\", \n",
    "                                       ascending=False).drop_duplicates('userId', keep='first').reset_index(drop=True)\n",
    "    df.index = df['userId']\n",
    "      \n",
    "            \n",
    "    return df['score'].to_dict()\n",
    "\n",
    "def mm_requestId(x):\n",
    "    if type(x)==dict and 'NOTE' in x.keys():\n",
    "        if x['NOTE'].find('HTTP_RESPONSE_BODY') >0:\n",
    "            htr = json.loads(x['NOTE'][x['NOTE'].find('HTTP_RESPONSE_BODY')+20:])\n",
    "            htr = htr['requestId'] if 'requestId' in htr.keys() else np.nan\n",
    "#             print(htr)\n",
    "        else: \n",
    "            htr = np.nan\n",
    "    else: \n",
    "        htr = np.nan\n",
    "    return htr\n",
    "\n",
    "def get_exact_claims(since, till, client):\n",
    "    req = {\"createdAt\": {\"$gte\": datetime.combine(since, datetime.min.time()) - timedelta(hours=3),\n",
    "                          \"$lt\": datetime.combine(till, datetime.min.time())- timedelta(hours=3)},\n",
    "#            \"product._id\": {\"$in\": [ObjectId(\"5e9846be79dd27000141f476\"),\n",
    "# ObjectId(\"5ef4473acf22a90001010445\")]},\n",
    "           #\"product._id\": {\"$in\": [ObjectId(product) for product in products]}}\n",
    "            \"status\": {\"$nin\": [ \"ERROR\", \"NEW\"]}} \n",
    "    results_count = client['claim-service']['claims'].count_documents(req)\n",
    "    # print(req)\n",
    "    print(results_count)\n",
    "    all_ans = []\n",
    "    batch_size = 10000\n",
    "    for i in range(results_count//batch_size+1):\n",
    "        ans = list(client['claim-service']['claims'].find(req, {\"product\": 1, \"partner\":1,\n",
    "                                                                \"borrower\": 1, \"statusFields\":1,\n",
    "                                                                \"status\": 1, \"adapterName\": 1,\n",
    "                                                                \"createdAt\": 1, 'type':1,\n",
    "                                                                \"updatedAt\":1, \"clickId\": 1,\n",
    "                                                                \"whence\":1}).limit(batch_size).skip(i*batch_size))\n",
    "        all_ans = all_ans + ans\n",
    "    #ans = trans(all_ans, client)\n",
    "    df = pd.DataFrame(all_ans)\n",
    "    df['id'] = df[\"_id\"].apply(lambda x: str(x))\n",
    "    df['dt'] = df[\"createdAt\"].apply(lambda x: (x+timedelta(hours=3)).date())\n",
    "    df['productId'] = df['product'].apply(lambda x: str(x['_id']))\n",
    "    df['product_type'] = df['product'].apply(lambda x: x['productType'] if 'productType' in x.keys() else np.nan)\n",
    "    df['product_name'] = df['product'].apply(lambda x: x['name'])\n",
    "    df['partner'] = df['partner'].apply(lambda x: x['name'])\n",
    "    df['userId'] = df['borrower'].apply(lambda x: x['userId'])\n",
    "    df['phone'] = df['borrower'].apply(lambda x: x['phone'] if 'phone' in x.keys() else np.nan)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_kibana_claims(es, since,till):\n",
    "    gte = (str(datetime.combine(since, datetime.min.time()).timestamp()).replace(\".\", \"\")+\"0000000\")[:13]\n",
    "    lt = (str(datetime.combine(till, datetime.min.time()).timestamp()).replace(\".\", \"\")+\"0000000\")[:13]\n",
    "\n",
    "    payload = {\"size\": 10000,\n",
    "\"query\":{\"bool\":{\"must\":[{\"match_all\":{}},\n",
    "                         {\"match_phrase\":{\"eventType.keyword\":\"CLAIM\"}},\n",
    "#                          {\"match_phrase\":{\"params.sourceGlobal.keyword\":{\"query\": sourceGlobal }}},\n",
    "                         {\"range\":{\"timestamp\":{\"gte\":gte,\"format\":\"epoch_millis\"}}}\n",
    "                        ]}},\n",
    "\"_source\": [\"params.parsed\",'params.clickId', 'params.userId', 'params.utm_campaign', 'params.utm_medium', \n",
    "            'params.utm_content','params.utm_source', 'params.utm_term', 'params.gaId', 'params.yandexClientID',\n",
    "            'params.sourceGlobal',\"timestamp\"]}\n",
    "              \n",
    "    \n",
    "    d = es.search(index='events', doc_type='event', body=payload, scroll='120m', request_timeout=300)\n",
    "    sleep(0.5)\n",
    "    doc_count = d['hits']['total']\n",
    "    hits = d['hits']['hits']\n",
    "    print(doc_count, end =\"; \", flush=True)\n",
    "    res = []\n",
    "    for i in range(1, int(doc_count/10000)+1):\n",
    "        scroll = d['_scroll_id']\n",
    "        d = es.scroll(scroll_id = scroll, scroll = '120m', request_timeout=300)\n",
    "        hits = hits + d['hits']['hits']\n",
    "        print(doc_count-10000*i, end =\"; \", flush=True)\n",
    "        sleep(0.5)\n",
    "\n",
    "    dr = pd.DataFrame([hit['_source'] for hit in hits])\n",
    "    dr.params.apply(lambda x: x.keys())\n",
    "    for col in ['gaId', 'userId', 'utm_campaign', 'utm_medium', 'utm_source', 'utm_term', 'productType',\n",
    "                'yandexClientID', 'productId', 'whence', 'transitionId', 'partnerName', #'payout', 'status',\n",
    "                  'transaction_id', 'clickId', 'sourceGlobal', 'utm_content','orderId','transactionId']:\n",
    "        dr[col] = dr['params'].apply(lambda x: x[col] if col in x.keys() else np.nan)\n",
    "\n",
    "    dr['time'] = dr['timestamp'].apply(lambda x: datetime.strptime(x[:19], '%Y-%m-%dT%H:%M:%S'))\n",
    "    dr['date'] = dr['time'].apply(lambda x: (x + timedelta(seconds=3*3600)).date())\n",
    "    return dr\n",
    "\n",
    "def get_kibana_postback(es, since):\n",
    "    gte = (str(datetime.combine(since, datetime.min.time()).timestamp()).replace(\".\", \"\")+\"0000000\")[:13]\n",
    "    lt = (str(datetime.combine(date.today()+timedelta(days=1), datetime.min.time()).timestamp()).replace(\".\", \"\")+\"0000000\")[:13]\n",
    "\n",
    "    payload = {\"size\": 10000,\n",
    "\"query\":{\"bool\":{\"must\":[{\"match_all\":{}},\n",
    "                         {\"match_phrase\":{\"eventType.keyword\":\"POSTBACK\"}},\n",
    "#                          {\"match_phrase\":{\"params.whence.keyword\":{\"query\": whence }}},\n",
    "                         {\"range\":{\"timestamp\":{\"gte\":gte,\"lt\":lt,\"format\":\"epoch_millis\"}}}]}}}\n",
    "    \n",
    "    d = es.search(index='events', doc_type='event', body=payload, scroll='120m', request_timeout=300)\n",
    "    sleep(0.5)\n",
    "    doc_count = d['hits']['total']\n",
    "    hits = d['hits']['hits']\n",
    "    print(doc_count, end =\"; \", flush=True)\n",
    "    res = []\n",
    "    for i in range(1, int(doc_count/10000)+1):\n",
    "        scroll = d['_scroll_id']\n",
    "        d = es.scroll(scroll_id = scroll, scroll = '120m', request_timeout=300)\n",
    "        hits = hits + d['hits']['hits']\n",
    "        print(doc_count-10000*i, end =\"; \", flush=True)\n",
    "        sleep(0.5)\n",
    "\n",
    "    dr = pd.DataFrame([hit['_source'] for hit in hits])\n",
    "    dr.params.apply(lambda x: x.keys())\n",
    "    for col in ['gaId', 'userId', 'utm_campaign', 'utm_medium', 'utm_source', 'utm_term', 'productType',\n",
    "                'yandexClientID', 'productId', 'whence', 'transitionId', 'partnerName', 'payout', 'status',\n",
    "                'transaction_id', 'externalId','orderId','transactionId']:\n",
    "        dr[col] = dr['params'].apply(lambda x: x[col] if col in x.keys() else np.nan)\n",
    "\n",
    "    dr['time'] = dr['timestamp'].apply(lambda x: datetime.strptime(x[:19], '%Y-%m-%dT%H:%M:%S'))\n",
    "    dr['date'] = dr['time'].apply(lambda x: (x + timedelta(seconds=3*3600)).date())\n",
    "    return dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kibana_claim_link(es, since,till):\n",
    "    gte = (str(datetime.combine(since, datetime.min.time()).timestamp()).replace(\".\", \"\")+\"0000000\")[:13]\n",
    "    lt = (str(datetime.combine(date.today()+timedelta(days=1), datetime.min.time()).timestamp()).replace(\".\", \"\")+\"0000000\")[:13]\n",
    "\n",
    "    payload = {\"size\": 10000,\n",
    "\"query\":{\"bool\":{\"must\":[{\"match_all\":{}},\n",
    "                         {\"match_phrase\":{\"eventType.keyword\":\"CLAIM_LINK\"}},\n",
    "#                          {\"match_phrase\":{\"params.whence.keyword\":{\"query\": whence }}},\n",
    "                         {\"range\":{\"timestamp\":{\"gte\":gte,\"lt\":lt,\"format\":\"epoch_millis\"}}}]}},\n",
    "    \"_source\": [\"params.claimId\",\"params.clickId\"]}\n",
    "    d = es.search(index='events', doc_type='event', body=payload, scroll='120m', request_timeout=300)\n",
    "    sleep(0.5)\n",
    "    doc_count = d['hits']['total']\n",
    "    hits = d['hits']['hits']\n",
    "    print(doc_count, end =\"; \", flush=True)\n",
    "    res = []\n",
    "    for i in range(1, int(doc_count/10000)+1):\n",
    "        scroll = d['_scroll_id']\n",
    "        d = es.scroll(scroll_id = scroll, scroll = '120m', request_timeout=300)\n",
    "        hits = hits + d['hits']['hits']\n",
    "        print(doc_count-10000*i, end =\"; \", flush=True)\n",
    "        sleep(0.5)\n",
    "\n",
    "    dr = pd.DataFrame([hit['_source'] for hit in hits])\n",
    "    dr.params.apply(lambda x: x.keys())\n",
    "    for col in ['claimId','clickId']:\n",
    "        dr[col] = dr['params'].apply(lambda x: x[col] if col in x.keys() else np.nan)\n",
    "    dr.params.apply(lambda x: x.keys())\n",
    "\n",
    "\n",
    "#     dr['time'] = dr['timestamp'].apply(lambda x: datetime.strptime(x[:19], '%Y-%m-%dT%H:%M:%S'))\n",
    "#     dr['date'] = dr['time'].apply(lambda x: (x + timedelta(seconds=3*3600)).date())\n",
    "    return dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = datetime(2021,1,4)\n",
    "till = datetime(2021,12,31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_exact_claims(since, till, client)\n",
    "df['requestId'] = df[df['partner']=='MoneyMan']['statusFields'].apply(mm_requestId)\n",
    "dpbk = get_kibana_postback(es, since)\n",
    "def float_apply(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return None\n",
    "dpbk['payout'] = dpbk['payout'].apply(float_apply)\n",
    "dpbk['payout'] = dpbk.apply(lambda x: x['payout']*1.2 if x['partnerName']=='admitad' else x['payout'], axis=1)\n",
    "dpbk = dpbk[dpbk['status'].isin(['APPROVED',\n",
    "                                 'approved',\n",
    "                                 'ISSUED'])].sort_values('payout').drop_duplicates('transitionId',\n",
    "                                                                                   keep='last')\n",
    "dr = pd.merge(left=df, right=dpbk, left_on='id', right_on='transitionId', how='left')\n",
    "dc = get_kibana_claims(es, since,till)[['clickId', 'userId', 'utm_campaign', 'utm_medium', 'utm_content', \n",
    "                                        'utm_source', 'utm_term', 'gaId',\n",
    "                                        'yandexClientID', 'sourceGlobal']]\n",
    "dr = pd.merge(left=dr, right=dc[dc['clickId'].notnull()].drop_duplicates('clickId'),\n",
    "              on='clickId', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = get_kibana_claim_link(es, since,till)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(dc['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = dc[dc['clickId'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr1 = dr.merge(dc,how='left',on='clickId')\n",
    "dr1=dr1[dr1['status_x']!='ACCEPTED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr1 = dr1[['dt','product_type','claimId','payout']]\n",
    "dr1.columns = ['dt','product_type','claimid','payout']\n",
    "dr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr1.to_sql('postback_app',engine,if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
